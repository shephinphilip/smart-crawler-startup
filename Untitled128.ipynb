{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCfJX7T8_00F",
        "outputId": "c63ecd74-4e69-4132-f380-febc533f0792"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax49QnMv_lUI",
        "outputId": "89a7f7b9-525f-4b36-8270-1512a704e5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¤ Crawling letter 'A' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'B' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'C' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'D' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'E' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'F' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'G' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'H' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'I' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'J' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'K' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'L' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'M' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'N' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'O' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'P' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'Q' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 14 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'R' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'S' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'T' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'U' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'V' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'W' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'X' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 13 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'Y' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "ðŸ”¤ Crawling letter 'Z' with 1 pages\n",
            "ðŸ“„  Page 1/1\n",
            "   ðŸ§± Found 15 startups\n",
            "\n",
            "âœ… Finished. Extracted total 387 startups\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import string\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://startupmahakumbh.org/exhibitor_directory/exhi_list_pub.php\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "def extract_startup_info(detail_div):\n",
        "    def get_value(detail_div, field):\n",
        "        for p in detail_div.find_all(\"p\"):\n",
        "            strong = p.find(\"strong\")\n",
        "            if strong and strong.text.strip().startswith(field):\n",
        "                return p.get_text(strip=True).replace(strong.text, \"\").strip()\n",
        "        return \"\"\n",
        "\n",
        "    name_tag = detail_div.find(\"h2\")\n",
        "    name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
        "    website = name_tag.find(\"a\")[\"href\"] if name_tag and name_tag.find(\"a\") else \"\"\n",
        "\n",
        "    return {\n",
        "        \"Name\": name,\n",
        "        \"Contact Person\": get_value(detail_div, \"Contact Person:\"),\n",
        "        \"Designation\": get_value(detail_div, \"Designation:\"),\n",
        "        \"Email\": get_value(detail_div, \"Contact Details:\"),\n",
        "        \"Profile\": get_value(detail_div, \"Profile:\"),\n",
        "        \"Website\": website\n",
        "    }\n",
        "\n",
        "\n",
        "def get_total_pages(soup):\n",
        "    page_input = soup.find(\"input\", {\"id\": \"currentPage\"})\n",
        "    if page_input and page_input.has_attr(\"max\"):\n",
        "        try:\n",
        "            return int(page_input[\"max\"])\n",
        "        except:\n",
        "            return 1\n",
        "    return 1\n",
        "\n",
        "def crawl_letter(letter):\n",
        "    startups = []\n",
        "    first_page_url = f\"{BASE_URL}?event_name=sm&event_year=2025&filter={letter}&page=1\"\n",
        "    response = requests.get(first_page_url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    total_pages = get_total_pages(soup)\n",
        "\n",
        "    print(f\"\\nðŸ”¤ Crawling letter '{letter}' with {total_pages} pages\")\n",
        "\n",
        "    for page in range(1, total_pages + 1):\n",
        "        url = f\"{BASE_URL}?event_name=sm&event_year=2025&filter={letter}&page={page}\"\n",
        "        print(f\"ðŸ“„  Page {page}/{total_pages}\")\n",
        "        res = requests.get(url, headers=HEADERS)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        detail_blocks = soup.select(\"div.col-md-10.col-sm-12 > div.details\")\n",
        "        print(f\"   ðŸ§± Found {len(detail_blocks)} startups\")\n",
        "\n",
        "        for detail_div in detail_blocks:\n",
        "            startup = extract_startup_info(detail_div)\n",
        "            if startup[\"Name\"]:  # Avoid blank entries\n",
        "                startups.append(startup)\n",
        "\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    return startups\n",
        "\n",
        "def crawl_all_letters():\n",
        "    all_startups = []\n",
        "    for letter in string.ascii_uppercase:\n",
        "        letter_data = crawl_letter(letter)\n",
        "        all_startups.extend(letter_data)\n",
        "\n",
        "    with open(\"startup_directory.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        fieldnames = [\"Name\", \"Contact Person\", \"Designation\", \"Email\", \"Profile\", \"Website\"]\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_startups)\n",
        "\n",
        "    print(f\"\\nâœ… Finished. Extracted total {len(all_startups)} startups\")\n",
        "\n",
        "# ðŸš€ Fire it up\n",
        "crawl_all_letters()\n"
      ]
    }
  ]
}